#!/usr/bin/env python


##### ##### ##### ##### ##### ##### #####
#                                       #
#                 graftM                #
#                                       #
#  A pipeline for gene centric analyses #
#          of metagenome datasets       #
#                                       #
##### ##### ##### ##### ##### ##### #####

__author__ = "Joel Boyd, Ben Woodcroft"
__copyright__ = "Copyright 2014"
__credits__ = ["Joel Boyd", "Ben Woodcroft"]
__license__ = "GPL3"
__maintainer__ = "Joel Boyd, Ben Woodcroft"
__email__ = "joel.boyd near uq.net.au, b.woodcroft near uq.edu.au"
__status__ = "Development"
__version__ = "0.3.5"

import argparse
import re
try:
    from Bio import SeqIO
except ImportError:
    print "Please install Biopython first"
    exit(1)
import subprocess
from datetime import datetime
from collections import OrderedDict
import shutil 
import timeit
import tempfile
import StringIO
import os
import IPython

# Constants - don't change them evar.
FORMAT_FASTA = 'FORMAT_FASTA'
FORMAT_FASTQ_GZ = 'FORMAT_FASTQ_GZ'

##### Classes #####

class Messenger:
    
    def __init__(self):
        self.time = datetime.now().strftime('%H:%M:%S')
        self.date = datetime.now().strftime('%d-%m-%Y')
        
    def message(self, message):
        print '  %s [%s]: %s' % (self.date, self.time, message)
    
    def error_message(self, message):
        print '  %s [%s]: == ERROR == %s' % (self.date, self.time, message)

class GraftMFiles:

    def __init__(self, old_title, out_path):

        if '/' in old_title:
            self.basename = old_title.split('/')[len(old_title.split('/')) - 1].split('.')[0]

        else:
            self.basename = old_title.split('.')[0]

        self.out_path = out_path

    def forward_read_hmmsearch_output_path(self):
        return os.path.join(self.out_path, "%s_for.hmmout.csv" % self.basename)

    def jplace_output_path(self):
        return "placements.jplace"
    
    def euk_free_path(self):
        return os.path.join(self.out_path, "%s_euk_free.fa" % self.basename)
    
    def euk_contam_path(self):
        return os.path.join(self.out_path, "%s_euk_contam.txt" % self.basename)

    def guppy_file_output_path(self):
        return ".guppy"

    def summary_table_output_path(self):
        return "_count_table.txt"
    
    def aligned_fasta_output_path(self):
        return os.path.join(self.out_path, "%s.aln.fasta" % self.basename)

    def orf_output_path(self):
        return os.path.join(self.out_path, "%s.orf" % self.basename)

    def orf_titles_output_path(self):
        return os.path.join(self.out_path, "%s.orf.titles" % self.basename)

    def orf_fasta_output_path(self):
        return os.path.join(self.out_path, "%s_orf.fa" % self.basename)

    def conv_output_for_path(self):
        return os.path.join(self.out_path, "%s_conv_for.faa" % self.basename)

    def conv_output_path(self):
        return os.path.join(self.out_path, "%s_conv.faa" % self.basename)

    def conv_output_rev_path(self):
        return os.path.join(self.out_path, "%s_conv_rev.faa" % self.basename)

    def reverse_read_hmmsearch_output_path(self):
        return os.path.join(self.out_path, "%s_rev.hmmout.csv" % self.basename)

    def fna_output_path(self):
        return os.path.join(self.out_path, "%s_hits.fna" % self.basename)

    def readnames_output_path(self):
        return os.path.join(self.out_path, "%s_readnames.txt" % self.basename)

    def sto_for_output_path(self):
        return os.path.join(self.out_path, "%s.for.sto" % self.basename)
    
    def sto_rev_output_path(self):
        return os.path.join(self.out_path, "%s.rev.sto" % self.basename)
    
    def sto_output_path(self):
        return os.path.join(self.out_path, "%s.sto" % self.basename)

    def orf_hmmsearch_output_path(self):
        return os.path.join(self.out_path, "%s_orf.hmmout.csv" % self.basename)
    
    def basic_stats_path(self):
        return os.path.join(self.out_path, "%s_basic_stats.txt" % self.basename)

    def command_log_path(self):
        return os.path.join(self.out_path, "%s_commands_log.txt" % self.basename)

    def for_aln_path(self):
        return os.path.join(self.out_path, "%s_for_aln.fa" % self.basename)
        
    def rev_aln_path(self):
        return os.path.join(self.out_path, "%s_rev_aln.fa" % self.basename)
    
    def coverage_table_path(self):
        return '_coverage_table.txt'
      
    def base(self):
        return os.path.join(self.out_path, "%s" % self.basename)


class Run:
    
    def __init__(self, args, sequence_file_list, input_file_format, Gfiles):
        self.args = args
        self.sequence_file_list = sequence_file_list
        self.input_file_format = input_file_format
        self.Gfiles = Gfiles
        
    def protein_pipeline(self):
            
            # Start Timer
            start_all = timeit.default_timer()
            
            # Search for reads
            Messenger().message('Searching %s using %s' % (self.sequence_file_list[0], self.args.hmm_file))
            start = timeit.default_timer()
            
            hmm_search_output = hmmsearch(
                                        self.Gfiles.forward_read_hmmsearch_output_path(), 
                                        self.Gfiles.reverse_read_hmmsearch_output_path(),
                                        self.args.hmm_file, 
                                        self.sequence_file_list, 
                                        self.input_file_format, 
                                        self.args.type,
                                        self.args.threads,
                                        self.args.eval
                                            )
                                            
            stop = timeit.default_timer()
            search_t = str(int(round((stop - start), 0)) )
            
            
            # Interpret the results of the hmmsearch output
            Messenger().message('Reading results')
            evals, n_total_reads, rev_true = csv_to_titles(
                                                        hmm_search_output, 
                                                        self.args.type,
                                                        self.Gfiles.readnames_output_path()
                                                            )
            
            
            # Extract the hists from the original file
            Messenger().message('Extracting reads')
            start = timeit.default_timer()
            
            extract_from_raw_reads(
                                self.sequence_file_list[0], 
                                self.args.hmm_file, 
                                self.Gfiles.readnames_output_path(), 
                                self.input_file_format, 
                                self.args.output_directory,
                                self.Gfiles.fna_output_path(),
                                self.Gfiles.orf_output_path(),
                                self.Gfiles.orf_hmmsearch_output_path(),
                                self.Gfiles.orf_titles_output_path(),
                                self.Gfiles.orf_fasta_output_path(),
                                False
                                    )  # And extract from the original file
            
            stop = timeit.default_timer()
            extract_t = str(int(round((stop - start), 0)) )
            
            
            # Align the reads
            Messenger().message('Aligning reads to reference package database')
            start = timeit.default_timer()
            
            hmmalign(
                    self.args.hmm_file, 
                    self.Gfiles.orf_fasta_output_path(), 
                    evals, 
                    rev_true,
                    self.Gfiles.sto_for_output_path(), 
                    self.Gfiles.for_aln_path(), 
                    self.Gfiles.conv_output_for_path(), 
                    self.Gfiles.sto_rev_output_path(), 
                    self.Gfiles.rev_aln_path(), 
                    self.Gfiles.conv_output_rev_path(), 
                    self.Gfiles.sto_output_path()
                        )  # And align
            
            alignment_correcter(
                            [self.Gfiles.conv_output_for_path()],
                            self.Gfiles.aligned_fasta_output_path()
                                )  # And fix up the alignment file by removing the insertions
            
            stop = timeit.default_timer()
            aln_t = str(int(round((stop - start), 0)) )
            
            return self.Gfiles.aligned_fasta_output_path(), start_all, n_total_reads, n_contamin_euks, n_uniq_euks, search_t, extract_t, euk_check_t, aln_t

    def dna_pipeline(self):
            # Start Timer
            start_all = timeit.default_timer()
            
            # Search for reads
            Messenger().message('Searching %s using %s' % (self.sequence_file_list[0], self.args.hmm_file))
            start = timeit.default_timer()
            
            hmm_search_output = nhmmer(
                                    self.Gfiles.forward_read_hmmsearch_output_path(), 
                                    self.Gfiles.reverse_read_hmmsearch_output_path(), 
                                    self.args.hmm_file, 
                                    self.sequence_file_list, 
                                    self.input_file_format,
                                    self.args.threads,
                                    self.args.eval
                                        )
                                        
            stop = timeit.default_timer()
            search_t = str(int(round((stop - start), 0)) )
            
            
            # Interpret the results of the nhmmer output
            Messenger().message('Reading results')
            evals, n_total_reads, rev_true = csv_to_titles(
                                                        hmm_search_output, 
                                                        self.args.type, 
                                                        self.Gfiles.readnames_output_path()
                                                            )
            
            
            # Extract the hists from the original file
            Messenger().message('Extracting reads')
            start = timeit.default_timer()
            
            extract_from_raw_reads(
                                self.sequence_file_list[0], 
                                self.args.hmm_file, 
                                self.Gfiles.readnames_output_path(), 
                                self.input_file_format, 
                                self.args.output_directory, 
                                self.Gfiles.fna_output_path(),
                                self.Gfiles.orf_output_path(),
                                self.Gfiles.orf_hmmsearch_output_path(),
                                self.Gfiles.orf_titles_output_path(),
                                self.Gfiles.orf_fasta_output_path(),
                                True
                                    )
            
            stop = timeit.default_timer()
            extract_t = str(int(round((stop - start), 0)) )
            
            
            # Check for Eukarytoic contamination
            Messenger().message("Checking for Eukaryotic contamination")
            start = timeit.default_timer()

            n_contamin_euks, n_uniq_euks = check_euk_contamination(
                                                                self.Gfiles.euk_free_path(),
                                                                self.Gfiles.euk_contam_path(), 
                                                                self.Gfiles.fna_output_path(), 
                                                                evals, 
                                                                check_read_length(self.Gfiles.fna_output_path()), 
                                                                self.input_file_format, 
                                                                args.threads, 
                                                                args.eval,
                                                                args.check_total_euks,
                                                                self.sequence_file_list[0]
                                                                    )
            stop = timeit.default_timer()
            euk_check_t = str( int(round((stop - start), 0)) )
            
                    
            # Align the reads
            Messenger().message('Aligning reads to reference package database')
            start = timeit.default_timer()
            
            if self.args.pynast_alignment:            
                n_failed_pynast = pynast(
                                        self.Gfiles.base(),
                                        self.Gfiles.aligned_fasta_output_path(), 
                                        self.args.GG_database, 
                                        self.Gfiles.euk_free_path()
                                            )
    
            
            else:
                hmmalign(
                        self.args.hmm_file, 
                        self.Gfiles.euk_free_path(), 
                        evals, rev_true, 
                        self.Gfiles.sto_for_output_path(), 
                        self.Gfiles.for_aln_path(), 
                        self.Gfiles.conv_output_for_path(), 
                        self.Gfiles.sto_rev_output_path(), 
                        self.Gfiles.rev_aln_path(), 
                        self.Gfiles.conv_output_rev_path(), 
                        self.Gfiles.sto_output_path()
                            )
                
                if rev_true:
                    alignment_correcter(
                                    [self.Gfiles.conv_output_for_path(), self.Gfiles.conv_output_rev_path()], 
                                    self.Gfiles.aligned_fasta_output_path()
                                        )
                
                else:
                    alignment_correcter(
                                    [self.Gfiles.conv_output_for_path()], 
                                    self.Gfiles.aligned_fasta_output_path()
                                        )
    
                    
            stop = timeit.default_timer()
            aln_t = str(int(round((stop - start), 0)) )
            
            return self.Gfiles.aligned_fasta_output_path(), start_all, n_total_reads, n_contamin_euks, n_uniq_euks, search_t, extract_t, euk_check_t, aln_t
            
    def placement(self, placement_files, base_list, start_all, n_total_reads, n_contamin_euks, n_uniq_euks, search_t, extract_t, euk_check_t, aln_t):
            # Check that there is a reference package to place in. If not, exit.     
            if self.args.skip_placement:
                Messenger().message('Stopping at alignment\n')
                delete([
                    self.Gfiles.for_aln_path(), 
                    self.Gfiles.rev_aln_path(), 
                    self.Gfiles.sto_for_output_path(), 
                    self.Gfiles.sto_rev_output_path(), 
                    self.Gfiles.conv_output_rev_path(), 
                    self.Gfiles.conv_output_for_path(), 
                    self.Gfiles.euk_free_path(), 
                    self.Gfiles.euk_contam_path(), 
                    self.Gfiles.readnames_output_path(), 
                    self.Gfiles.forward_read_hmmsearch_output_path(), 
                    self.args.output_directory+'/*pynast*',
                    self.Gfiles.sto_output_path(), 
                    self.Gfiles.reverse_read_hmmsearch_output_path(),
                    self.Gfiles.orf_titles_output_path(), 
                    self.Gfiles.orf_hmmsearch_output_path()
                        ])
                exit(0)
            

            # Place in tree
            Messenger().message('Placing reads into reference package tree')
            start = timeit.default_timer()
            pplacer(
                self.args.reference_package, 
                placement_files,
                self.Gfiles.jplace_output_path(),
                self.args.threads,
                base_list
                    )
            stop = timeit.default_timer()
            place_t = str(int(round((stop - start), 0)) )
            
            
            # Create Guppy File
            Messenger().message('Creating Guppy file')
            start = timeit.default_timer()
            n_placements = guppy_class(
                                    self.args.reference_package, 
                                    self.Gfiles.jplace_output_path(),
                                    self.Gfiles.guppy_file_output_path(),
                                    base_list
                                        )

            # Generate Summary Table
            
            base_path_list = [base+'/'+base for base in base_list]
            for idx, base in enumerate(base_list):
                
                Messenger().message('Building summary table for %s' % base)
                otu_builder(
                        base_path_list[idx] + self.Gfiles.guppy_file_output_path(), 
                        base_path_list[idx] + self.Gfiles.summary_table_output_path(),
                        self.args.placements_cutoff,
                        base
                            )
            
            
                # Generate coverage table
                Messenger().message('Building coverage table for %s' % base)
                coverage_of_hmm(
                            self.args.hmm_file, 
                            base_path_list[idx] + self.Gfiles.summary_table_output_path(), 
                            base_path_list[idx] + self.Gfiles.coverage_table_path(), 
                            check_read_length(self.Gfiles.fna_output_path())
                                )
            
            stop = timeit.default_timer()
            summary_t = str(int(round((stop - start), 0)) )
            
            # Compile basic run statistics
            stop_all = timeit.default_timer()
            all_t = str(int(round((stop_all - start_all), 0)) )

            
            
            # Delete unncessary files
            Messenger().message('Cleaning up')
            for base in base_list:
                Gfs = GraftMFiles(base,base)
                delete([
                    Gfs.for_aln_path(), 
                    Gfs.rev_aln_path(), 
                    Gfs.sto_for_output_path(), 
                    Gfs.sto_rev_output_path(), 
                    Gfs.conv_output_rev_path(), 
                    Gfs.conv_output_for_path(), 
                    Gfs.euk_free_path(), 
                    Gfs.euk_contam_path(), 
                    Gfs.readnames_output_path(), 
                    Gfs.forward_read_hmmsearch_output_path(), 
                    Gfs.sto_output_path(), 
                    Gfs.reverse_read_hmmsearch_output_path(),
                    Gfs.orf_titles_output_path(), 
                    Gfs.orf_hmmsearch_output_path(),
                    self.args.output_directory+'/*pynast*'
                        ])
            
            Messenger().message('Done, thanks for using graftM!\n')
            
            
##### Functions #####

def build_basic_statistics(output_path, n_total_16S_reads, n_contaminant_euk_reads, n_placed_reads, n_total_euk_reads, all_t, search_t, extract_t, euk_check_t, summary_t, aln_t, tree_i_t):
    
    if n_total_euk_reads == 'N/A':
        n_total_passed = n_total_16S_reads
        n_total = n_total_passed        
        p_contaminant_euk_reads = 'N/A'
        p_euk_reads = 'N/A'
        o = int(search_t) + int(extract_t) + int(tree_i_t) + int(summary_t) + int(aln_t)
        
        
    else:
        n_total_passed = n_total_16S_reads - n_contaminant_euk_reads
        n_total = n_total_passed + n_total_euk_reads
        p_euk_reads = str(round((float(n_total_euk_reads) / float(n_total)) * 100, 1)) + '%'
        p_contaminant_euk_reads = str(round((float(n_contaminant_euk_reads) / float(n_total)) * 100, 1)) + '%'
        o = int(search_t) + int(extract_t) + int(euk_check_t) + int(tree_i_t) + int(summary_t) + int(aln_t)
    

    other_t = str(int(all_t) - o)

    p_total_passed = str(round((float(n_total_passed) / float(n_total)*100), 1))+"%"
    p_placed_reads = str(round((float(n_placed_reads) / float(n_total_passed)*100), 1))+"%"

    

    
    stats = """Basic run statistics:

                                                        Number\tPercent total
    Total number of 16S/18S detected                    %s
    Total number of 16S reads detected:                 %s\t%s
    Total number of 18S reads detected:                 %s\t%s  
    
    
Placement statistics:

                                                        Number\tPercent 16S reads
    Total number of 'contaminant' eukaryotic reads:     %s\t%s
    Number of 16S reads placed in tree:                 %s\t%s
    
    
Runtime:

                            Time (seconds)
    Total runtime:          %s
    Search step:            %s
    Extract step:           %s
    Alignment step:         %s
    Eukaryote check step:   %s
    Tree insertion step     %s
    Summarising steps:      %s
    Other:                  %s
    
""" % (
    n_total, 
    n_total_passed, 
    p_total_passed, 
    n_total_euk_reads,
    p_euk_reads,
    n_contaminant_euk_reads,
    p_contaminant_euk_reads,
    n_placed_reads,
    p_placed_reads,
    all_t,
    search_t,
    extract_t,
    aln_t,
    euk_check_t,
    tree_i_t,
    summary_t,
    other_t
    )
    
    with open(output_path, 'w') as stats_file:
        stats_file.write(stats)
        


def check_read_length(reads):
    lengths = []
    
    for record in list(SeqIO.parse(open(reads, 'r'), 'fasta')):
        lengths.append(len(record.seq))
    return sum(lengths) / float(len(lengths))
    


def check_euk_contamination(euk_free_path, out_table, reads, evals, avg_read_length, input_file_format, threads, eval, check_total_euks, raw_reads):

    
    contamination_list = []
    euk_uniq = []
    cutoff = 0.9*avg_read_length
    # do a nhmmer using a Euk specific hmm
    
    
    if check_total_euks:
        nhmmer_cmd = "nhmmer --cpu %s %s --tblout %s /srv/db/graftm/0.1/HMM/Euk.hmm " % (threads, eval, out_table)
        
        if input_file_format == FORMAT_FASTA:
            cmd = nhmmer_cmd + raw_reads +' 2>&1 > /dev/null'

            # log
            subprocess.check_call(cmd, shell = True)
        
        elif input_file_format == FORMAT_FASTQ_GZ:
            cmd = nhmmer_cmd + " <(awk '{print \">\" substr($0,2);getline;print;getline;getline}' <(zcat " + raw_reads + ")) 2>&1 > /dev/null"
            # log
            subprocess.check_call(["/bin/bash", "-c", cmd])
    

    
        else:
            Messenger().error_message('Suffix on %s not familiar. Please submit an .fq.gz or .fa file\n' % (raw_reads))
            exit(1)
    
    else:    
        cmd = "nhmmer --cpu %s %s --tblout %s /srv/db/graftm/0.1/HMM/Euk.hmm %s 2>&1 > /dev/null " % (threads, eval, out_table, reads)
        # log
        subprocess.check_call(cmd, shell = True)
    
    
    
    # check for evalues that are lower, after eliminating hits with an alignment length of < 90% the length of the whole read.
    for line in open(out_table, 'r'):
        
        if not line.startswith('#'):
            
            line = line.split()
            try:
                
                if float(evals[line[0]][0]) >= float(line[12]):
                    
                    ali_length = float(line[6]) - float(line[7])
                    
                    if ali_length < 0:
                        ali_length = ali_length * -1.0
                    
                    if ali_length >= float(cutoff):
                        contamination_list.append(line[0])
                        euk_uniq.append(line[0])
            
            except KeyError:

                if check_total_euks:
                    euk_uniq.append(line[0])
                
                else:
                    continue
                
                
    # Return Euk contamination
    if len(contamination_list) == 0:
        Messenger().error_message("No contaminating eukaryotic reads detected")
    
    else:
        Messenger().message("Found %s read(s) that may be eukaryotic, continuing without it/them" % len(contamination_list))
    
    # Write a file with the Euk free reads.
    with open(euk_free_path, 'w') as euk_free_output:
    
        for record in list(SeqIO.parse(open(reads, 'r'), 'fasta')):

            if record.id not in contamination_list:
                SeqIO.write(record, euk_free_output, "fasta")

    return len(contamination_list), len(euk_uniq)
           
                


# Given a Return the guessed file format, or raise an Exception if
def guess_sequence_input_file_format(sequence_file_path):

    if sequence_file_path.endswith(('.fa', '.faa', '.fna')):  # Check the file type
        return FORMAT_FASTA

    elif sequence_file_path.endswith(('.fq.gz', '.fastq.gz')):
        return FORMAT_FASTQ_GZ

    else:
        raise Exception("Unable to guess file format of sequence file: %s" % sequence_file_path)



# Corrects sequence alignments by removing lower case insertions

def alignment_correcter(alignment_file_list, output_file_name):
    
    corrected_sequences = {}
    
    for alignment_file in alignment_file_list:
        insert_list = [] # Define list containing inserted positions to be removed (lower case characters)
        sequence_list = list(SeqIO.parse(open(alignment_file, 'r'), 'fasta'))
        for sequence in sequence_list: # For each sequence in the alignment
    
            for idx, nt in enumerate(list(sequence.seq)): # For each nucleotide in the sequence
    
                if nt.islower(): # Check for lower case character
                    insert_list.append(idx) # Add to the insert list if it is
    
        insert_list = list(OrderedDict.fromkeys(sorted(insert_list, reverse = True))) # Reverse the list and remove duplicate positions
    
        
    
        for sequence in sequence_list: # For each sequence in the alignment
            new_seq = list(sequence.seq) # Define a list of sequences to be iterable list for writing
    
            for position in insert_list: # For each position in the removal list
                del new_seq[position] # Delete that inserted position in every sequence
            
            corrected_sequences['>'+sequence.id+'\n'] = ''.join(new_seq)+'\n'
    
    
    with open(output_file_name, 'w') as output_file: # Create an open file to write the new sequences to
        for fasta_id, fasta_seq in corrected_sequences.iteritems():
            output_file.write(fasta_id)
            output_file.write(fasta_seq)
            

# Displays message with time in brackets before the message.

def message(message):
    time = datetime.now().strftime('%H:%M:%S')

    print '[' + time + ']: ' + str(message)

# split_names
def title_cleaner(title):
    d = title.split('_')
    del d[-1]
    return '_'.join(d)

# run nhmmer
def nhmmer(for_out_path, rev_out_path, hmm, sequence_file_list, input_file_format, threads, eval):
    suffix = [for_out_path, rev_out_path]
    table_title_list = []
    for seq_file in sequence_file_list:
        hmmout_table_title = suffix.pop(0)
        table_title_list.append(hmmout_table_title)
        nhmmer_cmd = "nhmmer --cpu %s %s --tblout %s %s" % (threads, eval, hmmout_table_title, hmm)
        
        if input_file_format == FORMAT_FASTA:
            cmd = "%s %s 2>&1 > /dev/null" % (nhmmer_cmd, seq_file)
            # log

            subprocess.check_call(["/bin/bash", "-c", cmd])

        elif input_file_format == FORMAT_FASTQ_GZ:
            cmd = "%s <(awk '{print \">\" substr($0,2);getline;print;getline;getline}' <(zcat %s)) 2>&1 > /dev/null" % (nhmmer_cmd, seq_file)
            subprocess.check_call(["/bin/bash", "-c", cmd])
            # log
        else:
            Messenger().message('ERROR: Suffix on %s not familiar. Please submit an .fq.gz or .fa file\n' % (seq_file))
            exit(1)

    return table_title_list

# run hmmsearch
def hmmsearch(for_out_reads, rev_out_reads, hmm, sequence_file_list, input_file_format, seq_type, threads, eval):
    suffix = [for_out_reads, rev_out_reads]
    table_title_list = []

    for seq_file in sequence_file_list:
        hmmout_table_title = suffix[0]
        table_title_list.append(hmmout_table_title)
        hmmsearch_cmd = " hmmsearch --cpu %s %s -o /dev/null --domtblout %s %s " % (threads, eval, hmmout_table_title, hmm)
        # TODO: capture stderr and report if the check_call fails

        if input_file_format == FORMAT_FASTA or input_file_format == FORMAT_FASTQ_GZ:

            if seq_type == 'P':
                cmd = 'orfm %s | %s /dev/stdin' % (seq_file, hmmsearch_cmd)
                # log
                subprocess.check_call(["/bin/bash", "-c", cmd])
            elif seq_type == 'D':

                if input_file_format == FORMAT_FASTQ_GZ:
                    cmd = "%s <(awk '{print \">\" substr($0,2);getline;print;getline;getline}' <(zcat %s)) 2>&1 > /dev/null " % (hmmsearch_cmd, seq_file)
                    # log
                    subprocess.check_call(["/bin/bash", "-c", cmd])

                elif input_file_format == FORMAT_FASTA:
                    cmd = "%s %s" % (hmmsearch_cmd, seq_file)
                    # log
                    subprocess.check_call(["/bin/bash", "-c", cmd])

            else:
                Messenger().message('ERROR: Programming error')
                exit(1)


        else:
            Messenger().message('ERROR: Suffix on %s not recegnised\n' % (seq_file))
            exit(1)
        del suffix[0]

    return table_title_list

# run pynast
def pynast(base, output_alignment, gg_db_path, input_sequences):
    
    cmd = 'pynast -l 0 -i %s -t %s -a %s' % (input_sequences, gg_db_path, output_alignment)
    # log
    subprocess.check_call(cmd, shell=True)
    
    n_failed_pynast = len(list(SeqIO.parse(open("%s_euk_free_pynast_fail.fasta" % base , 'r'), 'fasta')))
    
    return n_failed_pynast
    
# run pplacer
def pplacer(refpkg, ts_file, jplace_output, threads, base_list):

    cmd = "pplacer -j %s --verbosity 0 -c %s %s" % (threads, refpkg, ts_file)
    # log
    subprocess.check_call(cmd, shell=True)
    
    for base in base_list:
        dest = '%s/%s_%s' % (base, base, jplace_output)
        jplace_old = base + '.aln.jplace'
        shutil.move(jplace_old, dest)


# run guppy classify
def guppy_class(rpkg, jplace_file, guppy_path, base_list):

    placement_list = []
    guppy_main = 'Graftm' + guppy_path
    jplace_list = [x+'/'+x+'_placements.jplace' for x in base_list]

    cmd = 'guppy classify -c ' + rpkg + ' ' + ' '.join(jplace_list) + ' > ' + guppy_main
    # log
    subprocess.check_call(cmd, shell=True)
    
    
    # Split the guppy and distribute into ouput files
    guppy = []
    report = []
    
    for line in open(guppy_main, 'r'):
        
        if line.startswith('name'):
            if len(guppy) == 0:
                guppy.append(line)
            
            else:
                out =  guppy[1].split()[5].replace('_placements', '')
                r_num = 0
                seen = []
                for line in guppy:
                    if not line.startswith('name'):

                        seq_id = line.rstrip().split()[0]
                        if seq_id not in seen:
                            seen.append(seq_id)
                            r_num += 1
                
                report.append('%s\t%s' % (out, r_num))     
                out = out+'/'+out+'.guppy'
                
                with open(out, 'w') as out_guppy:
                    for line in guppy:
                        out_guppy.write(line)
                
                guppy = [] 
                guppy.append(line)
                        
        else:
            guppy.append(line)
    
    
    # Write the last guppy file
    
    out =  guppy[1].split()[5].replace('_placements', '')
    
    r_num = 0
    seen = []
    for line in guppy:
        if not line.startswith('name'):

            seq_id = line.rstrip().split()[0]
            if seq_id not in seen:
                seen.append(seq_id)
                r_num += 1
                
    report.append('%s\t%s' % (out, r_num))     

    out = out+'/'+out+'.guppy'
       
    with open(out, 'w') as out_guppy:
        for line in guppy:
            out_guppy.write(line)
       
    
    delete(['GraftM.guppy'])
    
    return report
    
# delete
def delete(delete_list):
    for item in delete_list:
        try:
            os.remove(item)
            
        except:
            pass
    
# HMM aligning
def hmmalign(hmm, sequencefile, evals, rev_true, sto_for_output_path, for_aln_path, conv_output_for_path, sto_rev_output_path, rev_aln_path, conv_output_rev_path, sto_output_path):


    # If there are reverse complement reads
    if rev_true:
        
        reverse = []
        forward = []
        
        records = list(SeqIO.parse(open(sequencefile), 'fasta'))
        
        # Split the reads into reverse and forward lists
        for record in records:
                
            if evals[record.id][1] == '+':
                forward.append(record)
                
            elif evals[record.id][1] == '-':
                reverse.append(record)
                
            else:
                Messenger().error_message('Programming error: hmmalign')
                exit(1)
            
            
        # Write reverse complement and forward reads to files
        with open(for_aln_path, 'w') as for_aln:
            for record in forward:
                for_aln.write('>'+record.id+'\n')
                for_aln.write(str(record.seq)+'\n')
            
        with open(rev_aln_path, 'w') as rev_aln:
            for record in reverse:
                rev_aln.write('>'+record.id+'\n')
                rev_aln.write(str(record.seq.reverse_complement())+'\n')
            
            
        # HMMalign and convert to fasta format
        cmd = 'hmmalign --trim -o %s %s %s 2>/dev/null; seqmagick convert %s %s  2>/dev/null' % (
                                                                                                sto_for_output_path, 
                                                                                                hmm, 
                                                                                                for_aln_path, 
                                                                                                sto_for_output_path, 
                                                                                                conv_output_for_path
                                                                                                    )
        # log
        subprocess.check_call(cmd, shell=True)
            
        cmd = 'hmmalign --trim -o %s %s %s 2>/dev/null; seqmagick convert %s %s  2>/dev/null' % (
                                                                                                sto_rev_output_path, 
                                                                                                hmm, 
                                                                                                rev_aln_path, 
                                                                                                sto_rev_output_path, 
                                                                                                conv_output_rev_path
                                                                                                )
        # log
        subprocess.check_call(cmd, shell=True)
        
    # If there are only forward reads, just hmm
    else:
        cmd = 'hmmalign --trim -o %s %s %s ; seqmagick convert %s %s' % (
                                                                        sto_output_path, 
                                                                        hmm, 
                                                                        sequencefile, 
                                                                        sto_output_path, 
                                                                        conv_output_for_path
                                                                        )
        # log
        subprocess.check_call(cmd, shell=True)
        
    
    
    

        
    
def csv_to_titles(hmm_table_list, graftm_pipeline, readnames_output_path):
    '''process hmmsearch/nhmmer results into a list of matching reads/ORFs for D/P respectively, to *_readnames.txt
    '''
    rev_true = False
    evals = {}
    titles_list = []
    reads_list = []
    write_list = []
    title_count = 0
    orfm_regex = re.compile('^(\S+)_(\d+)_(\d)_(\d+)')

    for hmm_table in hmm_table_list:

        for line in open(hmm_table):

            if line.startswith('#'):
                continue
            
            title_count += 1
            
            read_name = str(line.split()[0])
            e_value = line.split()[12]
            direction = line.split()[11]
            
            if direction == '-':
                rev_true = True
            
            if graftm_pipeline == 'D':

                titles_list.append(read_name)
                evals[read_name] = [e_value, direction]


            elif graftm_pipeline == 'P':
                # The original reads file contains sequences like
                # >eg and comment
                # where orfm gives orfs in the form of
                # >eg_1_2_3 and comment
                # The read_name here is the orfm style, we want to add to the titles_list
                # the original form
                regex_match = orfm_regex.match(read_name)

                if regex_match is not None:
                    titles_list.append(regex_match.groups(0)[0])

                else:
                    raise Exception("Unexpected form of ORF name found: %s" % read_name)

            else:
                raise Exception("Programming error")
        
        reads_list.append(titles_list)
        titles_list = []
    
    # Check if there arereads, and exit if non are found
    if title_count == 0:  
        Messenger().message('0 Reads found! Exiting')
        exit(0)  # and exit


         
    
    if len(reads_list) == 2:
        for_r = set(reads_list[0])

        for read in reads_list[1]:

            if read in for_r:
                write_list.append(read)

    elif len(reads_list) == 1:
        write_list = reads_list[0]

    Messenger().message('Found %s read(s) %s' % (len(write_list), hmm_table_list[0])) 
    
    with open(readnames_output_path, 'w') as output_file:

        for name in write_list:
    
            output_file.write(name + '\n')
    

    return evals, len(write_list), rev_true



# --- Creates a directory on bash command line

def make_working_directory(directory_path, force):

    if force:
        shutil.rmtree(directory_path, ignore_errors=True)
        os.mkdir(directory_path)
   
    else:
        try:
            os.mkdir(directory_path)
                
        except:
            Messenger().message('Directory %s already exists. Exiting to prevent over-writing\n' % directory_path)
            exit(1)    
    
# ---

def extract_from_raw_reads(raw_seq_file, hmm, name_file, input_file_format, outdir, sequence_file_name, raw_orf_title, hmm_out_title, orf_titles_path, orf_hmm_out_title, dna_pipe):
    # Run fxtract to obtain reads form original sequence file
    fxtract_cmd = "fxtract -H -X -f %s " % name_file
    if input_file_format == FORMAT_FASTA:
        cmd = fxtract_cmd + " " + raw_seq_file + " > " + sequence_file_name + ' '
        # log
        subprocess.check_call(cmd, shell=True)
    elif input_file_format == FORMAT_FASTQ_GZ:
        cmd = fxtract_cmd + raw_seq_file + " | awk '{print \">\" substr($0,2);getline;print;getline;getline}' > " + sequence_file_name + " "
        # log
        subprocess.check_call(fxtract_cmd + raw_seq_file + " | awk '{print \">\" substr($0,2);getline;print;getline;getline}' > " + sequence_file_name + " ", shell=True)
    else:
        raise Exception("Programming error")

    # Exit if in the dna pipeline
    if dna_pipe:
        return

    # Call orfs on the sequences
    cmd = 'orfm ' + sequence_file_name + ' > ' + raw_orf_title
    # log
    subprocess.check_call(cmd, shell=True)

    # Search for the correct reading fram
    cmd = "hmmsearch -o /dev/null --tblout " + hmm_out_title + " " + hmm + " " + raw_orf_title
    # log
    subprocess.check_call(cmd, shell=True)

    raw_titles = []

    for line in open(hmm_out_title):

        if line.startswith('#'):
            continue

        split = line.split(' ', 1)
        raw_titles.append(split[0])

     

    title_file_open = open(orf_titles_path, 'w')

    for title in raw_titles:
        title_file_open.write(str(title) + '\n')

    title_file_open.close()
    cmd = 'fxtract -H -X -f ' + orf_titles_path + ' ' + raw_orf_title + ' > ' + orf_hmm_out_title
    # log
    subprocess.check_call(cmd, shell=True)



# --- Builds the final otu table

def otu_builder(gup_file, output, cutoff, header):
    d = {}
    classifications = []
    placed = []
    otu_id = 0
    output_table = ['#ID\t'+header+'\tConsensusLineage']
    unique_list = []


    for line in open(gup_file, 'r'):
        list = line.split()

        if list[0] != 'name' and list[1] == list[2] and float(list[len(list)-2]) > float(cutoff):

            if list[0] not in d:
                d[list[0]] = []


            d[list[0]].append(list[3])

    for x,y in d.iteritems():

        if x not in placed:
            classifications.append(';'.join(y))
            placed.append(x)

        else:
            continue

    for x in classifications:

        if x not in unique_list:
            unique_list.append(x)

    for x in unique_list:
        output_table.append([str(otu_id),str(classifications.count(x)),x])
        otu_id += 1

    with open(output, 'w') as otu_table:

        for line in output_table:

            if '#' in line:
                otu_table.write(line+'\n')

            else:
                otu_table.write('\t'.join(line)+'\n')

def coverage_of_hmm(hmm, count_table, coverage_table, avg_read_length):
    
    for line in open(hmm):
        
        if line.startswith('LENG'):
            length = line.split()[1] 
    
    with open(coverage_table, 'w') as ct:
        write = ['#ID     20110816_S1D    ConsensusLineage\n']
        for line in open(count_table):
                
                if line.startswith('#'):
                    continue
                
                splt = line.split()

                cov = str(round((float(splt[1])*float(avg_read_length)) / float(length), 3))
                    
                write.append("%s\t%s\t%s\n" % (
                                splt[0],
                                cov,
                                splt[2]
                                ))
        
        for entry in write:
            ct.write(entry)
            
def parameter_checks(args):
    # --- Check parameters are in sensible land
    # Check that the necessary files are in place
    if hasattr(args, 'hmm_file') and not hasattr(args, 'reference_package'):
        Messenger().message('\nA reference package needs to be specified\n')
        exit(1)
    
    
    elif hasattr(args, 'reference_package') and not hasattr(args, 'hmm_file'):
        Messenger().message('A HMM file needs to be specified\n')
        exit(1)
    
        
    if args.pynast_alignment is True and not hasattr(args, 'GG_database'):
        Messenger().message('When using pynast alignments, an aligned file must be specifies using --GG_database')
        exit(1)
            
            
            
    # Check that the placement cutoff is between 0.5 and 1
    if float(args.placements_cutoff) < float(0.5) or float(args.placements_cutoff) > float(1.0):
        Messenger().message('Please specify a confidence level (-d) between 0.5 and 1.0! Found: %s' % args.placements_cutoff)
        exit(1)

    # Set string for hmmsearch evalue
    args.eval = '-E %s' % args.eval

    # Determine the File format based on the suffix
    input_file_format = guess_sequence_input_file_format(args.forward)
    
    # Make array of sequences

    sequence_file_list = []

    if hasattr(args, 'reverse'):
        fors = args.forward.split(',')
        revs = args.reverse.split(',')
        
        for i in range(0, len(fors)):
            sequence_file_list.append([fors[i], revs[i]])
        
    elif not hasattr(args, 'reverse'):
        for f in args.forward.split(','):
            f = [f]
            sequence_file_list.append(f)
            
    else:
        Messenger().error_message('Confusing input. Did you specify the same amount of reverse and forward read files?')
    
        
    return sequence_file_list, input_file_format

def reset_outdir(args, base):
    
    # reset working directory
    setattr(args, 'output_directory', base) 
    
    # create the directory
    make_working_directory(args.output_directory, args.force)
    

                    
            
def set_attributes(args, base):  
    
    # Read graftM packagea and assign HMM and refpkg file
       
    if hasattr(args, 'graftm_package'):
            
        for item in os.listdir(args.graftm_package):
                
            if item.endswith('.hmm'):
                setattr(args, 'hmm_file', os.path.join(args.graftm_package, item))
                    
            elif item.endswith('.refpkg'):
                setattr(args, 'reference_package', os.path.join(args.graftm_package, item))
        
        
    if not hasattr(args, 'reference_package') or not hasattr(args, 'hmm_file'):
        Messenger().message('ERROR: %s is empty or misformatted.' % args.graftm_package)
        exit(1)

        
        
def main(args):
    print '''
       #######################################
       ## graftM  %s                     ##
       ## Searches raw sequences for genes  ##
       ## Joel Boyd, Ben Woodcroft          ##
       #######################################

                                                 __/__
                                          ______|
  _- - _                         ________|      |_____/
   - -            -             |        |____/_
   - _     --->  -   --->   ____|
  - _-  -         -             |      ______
     - _                        |_____|
   -                                  |______
''' % __version__
        
    # Prepping - checking all necessary parameters are present, 
    # checking formats, making working directories

    set_attributes(args, GraftMFiles(args.forward, '').base())    
    sequence_pair_list, input_file_format = parameter_checks(args)
    placement_list = []
    base_list = []
    
    for pair in sequence_pair_list:       
         
        base = pair[0].split('.')[0]
        reset_outdir(args, base)
        print "\nWorking on %s" % base

        # Protein pipeline
        if args.type == 'P':
            placement_file, start_all, n_total_reads, n_contamin_euks, n_uniq_euks, search_t, extract_t, euk_check_t, aln_t = Run(args, pair, input_file_format, GraftMFiles(base, args.output_directory)).protein_pipeline()
        
        # DNA pipeline
        elif args.type == 'D':
            placement_file, start_all, n_total_reads, n_contamin_euks, n_uniq_euks, search_t, extract_t, euk_check_t, aln_t = Run(args, pair, input_file_format, GraftMFiles(base, args.output_directory)).dna_pipeline()
        
        placement_list.append(placement_file)
        base_list.append(base)
    print "\nGrafting..."
    Run(
        args, 
        pair, 
        input_file_format, 
        GraftMFiles(base, args.output_directory)).placement(
                                                        ' '.join(placement_list),
                                                        base_list,
                                                        start_all, 
                                                        n_total_reads, 
                                                        n_contamin_euks, 
                                                        n_uniq_euks, 
                                                        search_t, 
                                                        extract_t, 
                                                        euk_check_t, 
                                                        aln_t
                                                            )


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='''--- graftM %s --- Searches reads for genes using hmms, and places them into a tree with pplacer to classify them phylogenetically.''' % __version__
                                , epilog='Joel Boyd, Ben Woodcroft')
    parser.add_argument('--forward', metavar='forward read (or single read file)', type=str, help='comma separated list of forward reads .fa, or .fq.gz format.', required=True)
    parser.add_argument('--reverse', metavar='reverse read', type=str, help='[do NOT use unless you understand the difficulties with this] Optional reverse raw sequence file in .fa, or .fq.gz format.', default=argparse.SUPPRESS)
    parser.add_argument('--type', metavar='P or D', type=str, help='dna (like 16S) or prot (like mcrA)', choices=['P', 'D'], required=True)
    parser.add_argument('--eval', metavar='evalue', type=str, help='evalue cutoff for the hmmsearch (default = 1e-5)', default= '1e-5')
    parser.add_argument('--threads', metavar='threads', type=str, help='number of threads to use', default='5')
    parser.add_argument('--placements_cutoff', metavar='confidence', type=str, help='Cutoff of placement confidence level (0.5 - 1), default = 0.75', default=0.75)
    parser.add_argument('--graftm_package', metavar='reference_package', type=str, help='Reference package of gene family', default=argparse.SUPPRESS)
    parser.add_argument('--force', action="store_true", help='Force overwrite the output directory, even if one already exists with the same name', default=False)
    parser.add_argument('--pynast_alignment', action="store_true", help='Use hmmalign to align dna sequences.', default=False)
    parser.add_argument('--skip_placement', action="store_true", help='Stop after reads have been identified and aligned', default=False)
    parser.add_argument('--check_total_euks', action="store_true", help='Search whole sample for euks', default=False)
    parser.add_argument('--hmm_file', type=str, help='HMM file that hmmsearch uses', default=argparse.SUPPRESS)
    parser.add_argument('--GG_database', type=str, help='A green genes database, if pynast alignment is used', default=argparse.SUPPRESS)
    parser.add_argument('--reference_package', type=str, help='Reference package that pplacer uses', default=argparse.SUPPRESS)
    parser.add_argument('--output_directory', metavar='output directory', type=str, help='Specify an output directory (default is the file name) If you are looking for more than one marker gene in the same directory, you will definitely need this flag to avoid clobbering.', default=argparse.SUPPRESS)
    parser.add_argument('--version', action='version', version='graftM v%s' % __version__)
    
    args = parser.parse_args()
              
    main(args)

    exit(1)

    



